
---
title: 'Lab 7: Resampling Methods'
author: "Tuan Tran"
date: "10/31/2021"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Lab 7: Resampling Methods

You are required to submit two files: a knitted PDF/Word/HTML file that includes your solutions, results (R output), and your comments where needed, and a separate RMD file.

Submit your work via Blackboard.

We used logistic regression to predict the probability of
`default` using `income` and `balance` on the `Default` data set, which is part of the `ISLR2` package. You will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before
beginning your analysis.

```{r}
library(ISLR2)

```

@. Fit a logistic regression model that uses `income` and `balance` to
predict `default`.

```{r}
attach(Default)
model1 <- glm(default ~ income + balance, data=Default, family="binomial")

```


@. Using the validation set approach, estimate the test error of this
model. In order to do this, you must perform the following steps:

+ Split the sample set into a training set and a validation set.

```{r}
#split the sample into a training and validation
#I decided to slit 70% to the traiing set and 30% for testing set
set.seed(12)
sample <- sample(c(TRUE, FALSE), nrow(Default), replace = T, prob = c(0.7,0.3))
train <- Default[sample, ]
test  <- Default[!sample, ]

```

+ Fit a multiple logistic regression model using only the training observations.

```{r}
attach(Default)
model2 <- glm(default ~ income + balance, data=train,family="binomial")

```

+ Obtain a prediction of default status for each individual in
the validation set by computing the posterior probability of
default for that individual, and classifying the individual to
the `default` category if the posterior probability is greater
than 0.5.

```{r}

predict.prob <-predict(model2,test,type="response")

predict.value <-ifelse(predict.prob>0.5,"Yes","No")

```


+ Compute the validation set error, which is the fraction of
the observations in the validation set that are misclassified.

```{r}
table(test$default,predict.value)
error.original <-round(100*mean(predict.value!=test$default),2)
error.original
```


@. Repeat the process in (2) three times, using three different splits
of the observations into a training set and a validation set (you'll need to modify random seed). Comment on the results obtained.

```{r}

#Re-run first time
set.seed(34)  
#SPliting dataset 70-30
sample.1st <- sample(c(TRUE, FALSE), nrow(Default), replace = T, prob = c(0.7,0.3))
train.1st <- Default[sample.1st, ]
test.1st  <- Default[!sample.1st, ]
#Fit model
model.1st <- glm(default ~ income + balance, family=binomial, data=train.1st)
#Predicting probability 
predict.prob.1st <-predict(model.1st,test.1st,type="response")
#Obtaining predicted set of values for Default
predict.value.1st <-ifelse(predict.prob.1st>0.5,"Yes","No")
#Confusion matrix
table(test.1st$default,predict.value.1st)
#Calculating misclassification error
error.1st <- round(100*mean(predict.value.1st!=test.1st$default),2)
error.1st
```
```{r}
#Re-run second time 
#Splitting dataset
sample.2nd <- sample(c(TRUE, FALSE), nrow(Default), replace = T, prob = c(0.7,0.3))
train.2nd <- Default[sample.2nd, ]
test.2nd  <- Default[!sample.2nd, ]
#Fit model
model.2nd <- glm(default ~ income + balance, family="binomial", data=train.2nd)
#Predicting probability 
predict.prob.2nd <-predict(model.2nd,test.2nd,type="response")
#Obtaining predicted set of values for Default
predict.value.2nd <-ifelse(predict.prob.2nd>0.5,"Yes","No")
#Confusion matrix
table(test.2nd$default, predict.value.2nd)
#Calculating misclassification error
error.2nd <- round(100*mean(predict.value.2nd!=test.2nd$default),2)
error.2nd
```


```{r}
#Re-run third time
#Splitting dataset 80-20
sample.3rd <- sample(c(TRUE, FALSE), nrow(Default), replace = T, prob = c(0.7,0.3))
train.3rd <- Default[sample.3rd, ]
test.3rd  <- Default[!sample.3rd, ]
#Fit model
model.3rd <- glm(default ~ income + balance, family="binomial", data=train.3rd)
#Predicting probability 
predict.prob.3rd <-predict(model.3rd,test.3rd,type="response")
#Obtaining predicted set of values for Default
predict.value.3rd <-ifelse(predict.prob.3rd>0.5,"Yes","No")
#Confusion matrix
table(test.3rd$default, predict.value.3rd)
#Calculating misclassification error
error.3rd <- round(100*mean(predict.value.3rd!=test.3rd$default),2)
error.3rd
```

When we use 3 new diferent set.seed() value, we're able to obtain 3 new different error rates. While the 3rd error rate is hgiher than the original one, the error rates we obtained from 1st and 2nd try is smaller

@. Now consider a logistic regression model that predicts the probability
of `default` using `income`, `balance`, and a dummy variable
for `student`. Estimate the test error for this model using the validation set approach as in (1)-(2). Comment on whether or not including a dummy variable for `student` leads to a reduction in the test error rate.

```{r}
set.seed(910)  #New set seed function
#Splitting dataset
sample.new <- sample(c(TRUE, FALSE), nrow(Default), replace = T, prob = c(0.7,0.3))
train.new <- Default[sample.new, ]
test.new  <- Default[!sample.new, ]
#Fit model
model.stud <- glm(default ~ income + balance + student , family="binomial", data=train.new)
#Predicting probability 
predict.prob.stud <-predict(model.stud,test.new,type="response")
#Obtaining predicted set of values for Default
predict.value.stud <-ifelse(predict.prob.stud>0.5,"Yes","No")
#Confusion matrix
table(test.new$default, predict.value.stud)
#Calculating misclassification error
error.student <- round(100*mean(predict.value.stud!=test.new$default),2)
error.student
```

It doesn’t seem that adding the “student” dummy variable leads to a reduction in the validation set estimate of the test error rate. In fact, by adding the $student$ variables we increased the error rate compared to the original model